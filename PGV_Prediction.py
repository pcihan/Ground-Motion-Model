# -*- coding: utf-8 -*-
"""Pred-PGV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FzKdfRlcDZXWPqNgruZ6RenzGymYDWu0
"""

pip install xgboost

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import r2_score, mean_squared_error
import xgboost as xgb
from scipy.stats import pearsonr
import time

results = []
# ============================================================
# g1-PGV Dataset
# ============================================================
df = pd.read_excel("g1-PGV.xlsx")
X = df.drop(columns=[df.columns[-1]])
y = df[df.columns[-1]]
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ---- XGBoost (g1-PGV) ----
print("\n---- XGBoost (g1-PGV) ----")
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
rmse_list, r2_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    xgb_model.fit(X_train, y_train)
    y_pred = xgb_model.predict(X_test)
    fold_end_time = time.time()
    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    r, _ = pearsonr(y_test, y_pred)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    rmse_list.append(rmse); r2_list.append(r2); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g1-PGV","Model":"XGBoost","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Random Forest (g1-PGV) ----
print("\n---- Random Forest (g1-PGV) ----")
rf_model = RandomForestRegressor(random_state=42)
rmse_list, r2_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    rf_model.fit(X_train, y_train)
    y_pred = rf_model.predict(X_test)
    fold_end_time = time.time()
    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    r, _ = pearsonr(y_test, y_pred)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    rmse_list.append(rmse); r2_list.append(r2); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g1-PGV","Model":"Random Forest","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Gradient Boosting Regressor (g1-PGV) ----
print("\n---- Gradient Boosting Regressor (g1-PGV) ----")
gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
r2_list, rmse_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    gbr_model.fit(X_train, y_train)
    gbr_preds = gbr_model.predict(X_test)
    fold_end_time = time.time()
    r2 = r2_score(y_test, gbr_preds)
    rmse = np.sqrt(mean_squared_error(y_test, gbr_preds))
    mape = np.mean(np.abs((y_test - gbr_preds) / y_test)) * 100
    r, _ = pearsonr(y_test, gbr_preds)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    r2_list.append(r2); rmse_list.append(rmse); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g1-PGV","Model":"GBR","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Stacking-GBR (g1-PGV) ----
print("\n---- Stacking-GBR (g1-PGV) ----")
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
meta_model_gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
r2_list, rmse_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for train_index, test_index in kf.split(X):
    fold_start_time = time.time()
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    rf_model.fit(X_train, y_train)
    xgb_model.fit(X_train, y_train)
    rf_preds = rf_model.predict(X_test)
    xgb_preds = xgb_model.predict(X_test)
    stacked_preds = np.column_stack((rf_preds, xgb_preds))
    meta_model_gbr.fit(np.column_stack((rf_model.predict(X_train), xgb_model.predict(X_train))), y_train)
    final_preds = meta_model_gbr.predict(stacked_preds)
    fold_end_time = time.time()
    r2 = r2_score(y_test, final_preds)
    rmse = np.sqrt(mean_squared_error(y_test, final_preds))
    r, _ = pearsonr(y_test, final_preds)
    mape = np.mean(np.abs((y_test - final_preds) / y_test)) * 100
    print(f"Fold {len(r2_list)+1} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    r2_list.append(r2); rmse_list.append(rmse); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g1-PGV","Model":"Stacking-GBR","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ============================================================
# g2-PGV Dataset
# ============================================================
df = pd.read_excel("g2-PGV.xlsx")
X = df.drop(columns=[df.columns[-1]])
y = df[df.columns[-1]]
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ---- XGBoost (g2-PGV) ----
print("\n---- XGBoost (g2-PGV) ----")
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
rmse_list, r2_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    xgb_model.fit(X_train, y_train)
    y_pred = xgb_model.predict(X_test)
    fold_end_time = time.time()
    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    r, _ = pearsonr(y_test, y_pred)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    rmse_list.append(rmse); r2_list.append(r2); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g2-PGV","Model":"XGBoost","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Random Forest (g2-PGV) ----
print("\n---- Random Forest (g2-PGV) ----")
rf_model = RandomForestRegressor(random_state=42)
rmse_list, r2_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    rf_model.fit(X_train, y_train)
    y_pred = rf_model.predict(X_test)
    fold_end_time = time.time()
    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    r, _ = pearsonr(y_test, y_pred)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    rmse_list.append(rmse); r2_list.append(r2); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g2-PGV","Model":"Random Forest","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Gradient Boosting Regressor (g2-PGV) ----
print("\n---- Gradient Boosting Regressor (g2-PGV) ----")
gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
r2_list, rmse_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    gbr_model.fit(X_train, y_train)
    gbr_preds = gbr_model.predict(X_test)
    fold_end_time = time.time()
    r2 = r2_score(y_test, gbr_preds)
    rmse = np.sqrt(mean_squared_error(y_test, gbr_preds))
    mape = np.mean(np.abs((y_test - gbr_preds) / y_test)) * 100
    r, _ = pearsonr(y_test, gbr_preds)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    r2_list.append(r2); rmse_list.append(rmse); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g2-PGV","Model":"GBR","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Stacking-GBR (g2-PGV) ----
print("\n---- Stacking-GBR (g2-PGV) ----")
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
meta_model_gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
r2_list, rmse_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for train_index, test_index in kf.split(X):
    fold_start_time = time.time()
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    rf_model.fit(X_train, y_train)
    xgb_model.fit(X_train, y_train)
    rf_preds = rf_model.predict(X_test)
    xgb_preds = xgb_model.predict(X_test)
    stacked_preds = np.column_stack((rf_preds, xgb_preds))
    meta_model_gbr.fit(np.column_stack((rf_model.predict(X_train), xgb_model.predict(X_train))), y_train)
    final_preds = meta_model_gbr.predict(stacked_preds)
    fold_end_time = time.time()
    r2 = r2_score(y_test, final_preds)
    rmse = np.sqrt(mean_squared_error(y_test, final_preds))
    r, _ = pearsonr(y_test, final_preds)
    mape = np.mean(np.abs((y_test - final_preds) / y_test)) * 100
    print(f"Fold {len(r2_list)+1} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    r2_list.append(r2); rmse_list.append(rmse); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g2-PGV","Model":"Stacking-GBR","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ============================================================
# g3-PGV Dataset
# ============================================================
df = pd.read_excel("g3-PGV.xlsx")
X = df.drop(columns=[df.columns[-1]])
y = df[df.columns[-1]]
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ---- XGBoost (g3-PGV) ----
print("\n---- XGBoost (g3-PGV) ----")
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
rmse_list, r2_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    xgb_model.fit(X_train, y_train)
    y_pred = xgb_model.predict(X_test)
    fold_end_time = time.time()
    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    r, _ = pearsonr(y_test, y_pred)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    rmse_list.append(rmse); r2_list.append(r2); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g3-PGV","Model":"XGBoost","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Random Forest (g3-PGV) ----
print("\n---- Random Forest (g3-PGV) ----")
rf_model = RandomForestRegressor(random_state=42)
rmse_list, r2_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    rf_model.fit(X_train, y_train)
    y_pred = rf_model.predict(X_test)
    fold_end_time = time.time()
    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    r, _ = pearsonr(y_test, y_pred)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    rmse_list.append(rmse); r2_list.append(r2); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g3-PGV","Model":"Random Forest","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Gradient Boosting Regressor (g3-PGV) ----
print("\n---- Gradient Boosting Regressor (g3-PGV) ----")
gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
r2_list, rmse_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    gbr_model.fit(X_train, y_train)
    gbr_preds = gbr_model.predict(X_test)
    fold_end_time = time.time()
    r2 = r2_score(y_test, gbr_preds)
    rmse = np.sqrt(mean_squared_error(y_test, gbr_preds))
    mape = np.mean(np.abs((y_test - gbr_preds) / y_test)) * 100
    r, _ = pearsonr(y_test, gbr_preds)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    r2_list.append(r2); rmse_list.append(rmse); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g3-PGV","Model":"GBR","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Stacking-GBR (g3-PGV) ----
print("\n---- Stacking-GBR (g3-PGV) ----")
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
meta_model_gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
r2_list, rmse_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for train_index, test_index in kf.split(X):
    fold_start_time = time.time()
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    rf_model.fit(X_train, y_train)
    xgb_model.fit(X_train, y_train)
    rf_preds = rf_model.predict(X_test)
    xgb_preds = xgb_model.predict(X_test)
    stacked_preds = np.column_stack((rf_preds, xgb_preds))
    meta_model_gbr.fit(np.column_stack((rf_model.predict(X_train), xgb_model.predict(X_train))), y_train)
    final_preds = meta_model_gbr.predict(stacked_preds)
    fold_end_time = time.time()
    r2 = r2_score(y_test, final_preds)
    rmse = np.sqrt(mean_squared_error(y_test, final_preds))
    r, _ = pearsonr(y_test, final_preds)
    mape = np.mean(np.abs((y_test - final_preds) / y_test)) * 100
    print(f"Fold {len(r2_list)+1} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    r2_list.append(r2); rmse_list.append(rmse); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g3-PGV","Model":"Stacking-GBR","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ============================================================
# g4-PGV Dataset
# ============================================================
df = pd.read_excel("g4-PGV.xlsx")
X = df.drop(columns=[df.columns[-1]])
y = df[df.columns[-1]]
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ---- XGBoost (g4-PGV) ----
print("\n---- XGBoost (g4-PGV) ----")
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
rmse_list, r2_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    xgb_model.fit(X_train, y_train)
    y_pred = xgb_model.predict(X_test)
    fold_end_time = time.time()
    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    r, _ = pearsonr(y_test, y_pred)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    rmse_list.append(rmse); r2_list.append(r2); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g4-PGV","Model":"XGBoost","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Random Forest (g4-PGV) ----
print("\n---- Random Forest (g4-PGV) ----")
rf_model = RandomForestRegressor(random_state=42)
rmse_list, r2_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    rf_model.fit(X_train, y_train)
    y_pred = rf_model.predict(X_test)
    fold_end_time = time.time()
    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    r, _ = pearsonr(y_test, y_pred)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    rmse_list.append(rmse); r2_list.append(r2); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g4-PGV","Model":"Random Forest","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Gradient Boosting Regressor (g4-PGV) ----
print("\n---- Gradient Boosting Regressor (g4-PGV) ----")
gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
r2_list, rmse_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    gbr_model.fit(X_train, y_train)
    gbr_preds = gbr_model.predict(X_test)
    fold_end_time = time.time()
    r2 = r2_score(y_test, gbr_preds)
    rmse = np.sqrt(mean_squared_error(y_test, gbr_preds))
    mape = np.mean(np.abs((y_test - gbr_preds) / y_test)) * 100
    r, _ = pearsonr(y_test, gbr_preds)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    r2_list.append(r2); rmse_list.append(rmse); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g4-PGV","Model":"GBR","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Stacking-GBR (g4-PGV) ----
print("\n---- Stacking-GBR (g4-PGV) ----")
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
meta_model_gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
r2_list, rmse_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for train_index, test_index in kf.split(X):
    fold_start_time = time.time()
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    rf_model.fit(X_train, y_train)
    xgb_model.fit(X_train, y_train)
    rf_preds = rf_model.predict(X_test)
    xgb_preds = xgb_model.predict(X_test)
    stacked_preds = np.column_stack((rf_preds, xgb_preds))
    meta_model_gbr.fit(np.column_stack((rf_model.predict(X_train), xgb_model.predict(X_train))), y_train)
    final_preds = meta_model_gbr.predict(stacked_preds)
    fold_end_time = time.time()
    r2 = r2_score(y_test, final_preds)
    rmse = np.sqrt(mean_squared_error(y_test, final_preds))
    r, _ = pearsonr(y_test, final_preds)
    mape = np.mean(np.abs((y_test - final_preds) / y_test)) * 100
    print(f"Fold {len(r2_list)+1} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    r2_list.append(r2); rmse_list.append(rmse); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"g4-PGV","Model":"Stacking-GBR","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})
# ============================================================
# PGV Dataset
# ============================================================
df = pd.read_excel("PGV.xlsx")
X = df.drop(columns=[df.columns[-1]])
y = df[df.columns[-1]]
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ---- XGBoost (PGV) ----
print("\n---- XGBoost (PGV) ----")
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
rmse_list, r2_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    xgb_model.fit(X_train, y_train)
    y_pred = xgb_model.predict(X_test)
    fold_end_time = time.time()
    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    r, _ = pearsonr(y_test, y_pred)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    rmse_list.append(rmse); r2_list.append(r2); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"PGV","Model":"XGBoost","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Random Forest (PGV) ----
print("\n---- Random Forest (PGV) ----")
rf_model = RandomForestRegressor(random_state=42)
rmse_list, r2_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    rf_model.fit(X_train, y_train)
    y_pred = rf_model.predict(X_test)
    fold_end_time = time.time()
    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    r, _ = pearsonr(y_test, y_pred)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    rmse_list.append(rmse); r2_list.append(r2); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"PGV","Model":"Random Forest","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Gradient Boosting Regressor (PGV) ----
print("\n---- Gradient Boosting Regressor (PGV) ----")
gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
r2_list, rmse_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    fold_start_time = time.time()
    gbr_model.fit(X_train, y_train)
    gbr_preds = gbr_model.predict(X_test)
    fold_end_time = time.time()
    r2 = r2_score(y_test, gbr_preds)
    rmse = np.sqrt(mean_squared_error(y_test, gbr_preds))
    mape = np.mean(np.abs((y_test - gbr_preds) / y_test)) * 100
    r, _ = pearsonr(y_test, gbr_preds)
    print(f"Fold {fold} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    r2_list.append(r2); rmse_list.append(rmse); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"PGV","Model":"GBR","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ---- Stacking-GBR (PGV) ----
print("\n---- Stacking-GBR (PGV) ----")

r2_list, rmse_list, mape_list, r_list = [], [], [], []
total_start_time = time.time()
for train_index, test_index in kf.split(X):
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
    meta_model_gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)

    fold_start_time = time.time()
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    rf_model.fit(X_train, y_train)
    xgb_model.fit(X_train, y_train)

    rf_train_preds = rf_model.predict(X_train)
    xgb_train_preds = xgb_model.predict(X_train)
    train_stacked = np.column_stack((rf_train_preds, xgb_train_preds))

    meta_model_gbr.fit(train_stacked, y_train)

    rf_test_preds = rf_model.predict(X_test)
    xgb_test_preds = xgb_model.predict(X_test)
    stacked_test = np.column_stack((rf_test_preds, xgb_test_preds))

    final_preds = meta_model_gbr.predict(stacked_test)

    fold_end_time = time.time()
    r2 = r2_score(y_test, final_preds)
    rmse = np.sqrt(mean_squared_error(y_test, final_preds))
    r, _ = pearsonr(y_test, final_preds)
    mape = np.mean(np.abs((y_test - final_preds) / y_test)) * 100
    print(f"Fold {len(r2_list)+1} R²: {r2:.4f}, r: {r:.4f}, RMSE: {rmse:.3f}, Time: {fold_end_time - fold_start_time:.3f}s")
    r2_list.append(r2); rmse_list.append(rmse); mape_list.append(mape); r_list.append(r)

avg_r2 = np.mean(r2_list); avg_r = np.mean(r_list)
avg_rmse = np.mean(rmse_list); avg_mape = np.mean(mape_list)
exec_time = time.time() - total_start_time
print(f"\nAvg R²: {avg_r2:.4f}\nAvg r: {avg_r:.4f}\nAvg RMSE: {avg_rmse:.3f}\nAvg MAPE: {avg_mape:.1f}%")
print(f"Total Execution Time: {exec_time:.3f}s\n")
results.append({"Dataset":"PGV","Model":"Stacking-GBR","Avg_R2":avg_r2,"Avg_r":avg_r,"Avg_RMSE":avg_rmse,"Avg_MAPE":avg_mape,"Execution_Time":exec_time})

# ============================================================
# Save All Results to Excel
# ============================================================
results_df = pd.DataFrame(results)
results_df.to_excel("All_Results_PGV.xlsx", index=False)
print("\n All model results saved to 'All_Results_PGV.xlsx'")