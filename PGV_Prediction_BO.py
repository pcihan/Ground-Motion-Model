# -*- coding: utf-8 -*-
"""BO-pred_PGV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A1bg18sCyMf9Rsj2xsBy71nbpCbTin5Z
"""

pip install xgboost

pip install bayesian-optimization

# ============================================================
# BAYESIAN OPTIMIZATION for g1-PGV, g2-PGV, g3-PGV, g4-PGV, PGV datasets
# ============================================================
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
from scipy.stats import pearsonr
from bayes_opt import BayesianOptimization
import time

results = []

def run_models_for_dataset(dataset_name):

    print(f"\n==================== {dataset_name} ====================")
    df = pd.read_excel(f"{dataset_name}.xlsx")
    X = df.drop(columns=[df.columns[-1]])
    y = df[df.columns[-1]]
    kf = KFold(n_splits=5, shuffle=True, random_state=42)

    # --------------------- XGBoost ---------------------
    print(f"\n---- XGBoost ({dataset_name}) ----")

    def xgb_evaluate(learning_rate, max_depth, subsample, colsample_bytree, n_estimators):
        params = {
            "learning_rate": learning_rate,
            "max_depth": int(max_depth),
            "subsample": subsample,
            "colsample_bytree": colsample_bytree,
            "n_estimators": int(n_estimators),
            "objective": "reg:squarederror",
            "random_state": 42,
        }
        scores = []
        for train_index, test_index in kf.split(X):
            X_train, X_test = X.iloc[train_index], X.iloc[test_index]
            y_train, y_test = y.iloc[train_index], y.iloc[test_index]
            model = xgb.XGBRegressor(**params)
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            scores.append(r2_score(y_test, y_pred))
        return np.mean(scores)

    bayes_optimizer = BayesianOptimization(
        f=xgb_evaluate,
        pbounds={
            "learning_rate": (0.01, 0.1),
            "max_depth": (3, 5),
            "subsample": (0.6, 1.0),
            "colsample_bytree": (0.6, 1.0),
            "n_estimators": (100, 300),
        },
        random_state=42,
        verbose=0
    )
    bayes_optimizer.maximize(init_points=3, n_iter=5)
    best_params = bayes_optimizer.max["params"]
    best_params["max_depth"] = int(best_params["max_depth"])
    best_params["n_estimators"] = int(best_params["n_estimators"])
    print(f"Best Parameters (XGBoost - {dataset_name}): {best_params}")

    r2_list, r_list, rmse_list, mape_list = [], [], [], []
    start_time = time.time()
    for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
        fold_start = time.time()
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        model = xgb.XGBRegressor(**best_params, random_state=42)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r, _ = pearsonr(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / np.where(y_test == 0, 1e-8, y_test))) * 100
        print(f"Fold {fold}: RÂ²={r2:.4f}, r={r:.4f}, RMSE={rmse:.3f}, MAPE={mape:.1f}%, Time={time.time()-fold_start:.2f}s")
        r2_list.append(r2); r_list.append(r); rmse_list.append(rmse); mape_list.append(mape)
    exec_time = time.time() - start_time
    print(f"Avg RÂ²={np.mean(r2_list):.4f}, Avg r={np.mean(r_list):.4f}, Avg RMSE={np.mean(rmse_list):.3f}, Avg MAPE={np.mean(mape_list):.1f}%")
    results.append({"Dataset":dataset_name,"Model":"XGBoost","Avg_R2":np.mean(r2_list),"Avg_r":np.mean(r_list),
                    "Avg_RMSE":np.mean(rmse_list),"Avg_MAPE":np.mean(mape_list),"Execution_Time":exec_time})

    # --------------------- Random Forest ---------------------
    print(f"\n---- Random Forest ({dataset_name}) ----")

    def rf_evaluate(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features):
        params = {
            "n_estimators": int(n_estimators),
            "max_depth": int(max_depth),
            "min_samples_split": int(min_samples_split),
            "min_samples_leaf": int(min_samples_leaf),
            "max_features": int(max_features),
            "random_state": 42,
            "n_jobs": -1
        }
        scores = []
        for train_index, test_index in kf.split(X):
            X_train, X_test = X.iloc[train_index], X.iloc[test_index]
            y_train, y_test = y.iloc[train_index], y.iloc[test_index]
            model = RandomForestRegressor(**params)
            model.fit(X_train, y_train)
            scores.append(r2_score(y_test, model.predict(X_test)))
        return np.mean(scores)

    bayes_optimizer = BayesianOptimization(
        f=rf_evaluate,
        pbounds={
            "n_estimators": (100, 300),
            "max_depth": (3, 5),
            "min_samples_split": (2, 5),
            "min_samples_leaf": (1, 5),
            "max_features": (1, X.shape[1])
        },
        random_state=42,
        verbose=0
    )
    bayes_optimizer.maximize(init_points=3, n_iter=5)
    best_params = bayes_optimizer.max["params"]
    for p in ["n_estimators", "max_depth", "min_samples_split", "min_samples_leaf", "max_features"]:
        best_params[p] = int(best_params[p])
    print(f"Best Parameters (Random Forest - {dataset_name}): {best_params}")

    r2_list, r_list, rmse_list, mape_list = [], [], [], []
    start_time = time.time()
    for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
        fold_start = time.time()
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r, _ = pearsonr(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / np.where(y_test == 0, 1e-8, y_test))) * 100
        print(f"Fold {fold}: RÂ²={r2:.4f}, r={r:.4f}, RMSE={rmse:.3f}, MAPE={mape:.1f}%, Time={time.time()-fold_start:.2f}s")
        r2_list.append(r2); r_list.append(r); rmse_list.append(rmse); mape_list.append(mape)
    exec_time = time.time() - start_time
    print(f"Avg RÂ²={np.mean(r2_list):.4f}, Avg r={np.mean(r_list):.4f}, Avg RMSE={np.mean(rmse_list):.3f}, Avg MAPE={np.mean(mape_list):.1f}%")
    results.append({"Dataset":dataset_name,"Model":"Random Forest","Avg_R2":np.mean(r2_list),"Avg_r":np.mean(r_list),
                    "Avg_RMSE":np.mean(rmse_list),"Avg_MAPE":np.mean(mape_list),"Execution_Time":exec_time})

    # --------------------- Gradient Boosting ---------------------
    print(f"\n---- Gradient Boosting Regressor ({dataset_name}) ----")

    def gbr_evaluate(learning_rate, n_estimators, max_depth, subsample, min_samples_split, min_samples_leaf):
        params = {
            "learning_rate": learning_rate,
            "n_estimators": int(n_estimators),
            "max_depth": int(max_depth),
            "subsample": subsample,
            "min_samples_split": int(min_samples_split),
            "min_samples_leaf": int(min_samples_leaf),
            "random_state": 42,
        }
        scores = []
        for train_index, test_index in kf.split(X):
            X_train, X_test = X.iloc[train_index], X.iloc[test_index]
            y_train, y_test = y.iloc[train_index], y.iloc[test_index]
            model = GradientBoostingRegressor(**params)
            model.fit(X_train, y_train)
            scores.append(r2_score(y_test, model.predict(X_test)))
        return np.mean(scores)

    bayes_optimizer = BayesianOptimization(
        f=gbr_evaluate,
        pbounds={
            "learning_rate": (0.01, 0.1),
            "n_estimators": (100, 300),
            "max_depth": (2, 5),
            "subsample": (0.6, 1.0),
            "min_samples_split": (2, 5),
            "min_samples_leaf": (1, 3)
        },
        random_state=42,
        verbose=0
    )
    bayes_optimizer.maximize(init_points=3, n_iter=5)
    best_params = bayes_optimizer.max["params"]
    for p in ["n_estimators", "max_depth", "min_samples_split", "min_samples_leaf"]:
        best_params[p] = int(best_params[p])
    print(f"Best Parameters (GBR - {dataset_name}): {best_params}")

    r2_list, r_list, rmse_list, mape_list = [], [], [], []
    start_time = time.time()
    for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
        fold_start = time.time()
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        model = GradientBoostingRegressor(**best_params, random_state=42)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r, _ = pearsonr(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / np.where(y_test == 0, 1e-8, y_test))) * 100
        print(f"Fold {fold}: RÂ²={r2:.4f}, r={r:.4f}, RMSE={rmse:.3f}, MAPE={mape:.1f}%, Time={time.time()-fold_start:.2f}s")
        r2_list.append(r2); r_list.append(r); rmse_list.append(rmse); mape_list.append(mape)
    exec_time = time.time() - start_time
    print(f"Avg RÂ²={np.mean(r2_list):.4f}, Avg r={np.mean(r_list):.4f}, Avg RMSE={np.mean(rmse_list):.3f}, Avg MAPE={np.mean(mape_list):.1f}%")
    results.append({"Dataset":dataset_name,"Model":"GBR","Avg_R2":np.mean(r2_list),"Avg_r":np.mean(r_list),
                    "Avg_RMSE":np.mean(rmse_list),"Avg_MAPE":np.mean(mape_list),"Execution_Time":exec_time})

    # --------------------- Stacking-GBR ---------------------
    print(f"\n---- Stacking-GBR ({dataset_name}) ----")

    def stacking_evaluate(learning_rate, n_estimators, max_depth, subsample, min_samples_split, min_samples_leaf):
        params = {
            "learning_rate": learning_rate,
            "n_estimators": int(n_estimators),
            "max_depth": int(max_depth),
            "subsample": subsample,
            "min_samples_split": int(min_samples_split),
            "min_samples_leaf": int(min_samples_leaf),
            "random_state": 42,
        }
        scores = []
        for train_index, test_index in kf.split(X):
            X_train, X_test = X.iloc[train_index], X.iloc[test_index]
            y_train, y_test = y.iloc[train_index], y.iloc[test_index]

            rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
            xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

            rf_model.fit(X_train, y_train)
            xgb_model.fit(X_train, y_train)

            rf_train = rf_model.predict(X_train)
            xgb_train = xgb_model.predict(X_train)
            stacked_train = np.column_stack((rf_train, xgb_train))

            rf_test = rf_model.predict(X_test)
            xgb_test = xgb_model.predict(X_test)
            stacked_test = np.column_stack((rf_test, xgb_test))

            meta = GradientBoostingRegressor(**params)
            meta.fit(stacked_train, y_train)
            preds = meta.predict(stacked_test)

            scores.append(r2_score(y_test, preds))
        return np.mean(scores)

    bayes_optimizer = BayesianOptimization(
        f=stacking_evaluate,
        pbounds={
            "learning_rate": (0.01, 0.1),
            "n_estimators": (100, 300),
            "max_depth": (2, 5),
            "subsample": (0.6, 1.0),
            "min_samples_split": (2, 5),
            "min_samples_leaf": (1, 3),
        },
        random_state=42,
        verbose=0
    )
    bayes_optimizer.maximize(init_points=3, n_iter=5)
    best_params = bayes_optimizer.max["params"]
    for p in ["n_estimators", "max_depth", "min_samples_split", "min_samples_leaf"]:
        best_params[p] = int(best_params[p])
    print(f"Best Parameters (Stacking-GBR - {dataset_name}): {best_params}")

    r2_list, r_list, rmse_list, mape_list = [], [], [], []
    start_time = time.time()
    for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
        fold_start = time.time()
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
        xgb_model = xgb.XGBRegressor(objective="reg:squarederror", random_state=42)

        rf_model.fit(X_train, y_train)
        xgb_model.fit(X_train, y_train)

        rf_train = rf_model.predict(X_train)
        xgb_train = xgb_model.predict(X_train)
        stacked_train = np.column_stack((rf_train, xgb_train))

        rf_test = rf_model.predict(X_test)
        xgb_test = xgb_model.predict(X_test)
        stacked_test = np.column_stack((rf_test, xgb_test))

        meta_model = GradientBoostingRegressor(**best_params, random_state=42)
        meta_model.fit(stacked_train, y_train)
        preds = meta_model.predict(stacked_test)

        r2 = r2_score(y_test, preds)
        rmse = np.sqrt(mean_squared_error(y_test, preds))
        r, _ = pearsonr(y_test, preds)
        mape = np.mean(np.abs((y_test - preds) / np.where(y_test == 0, 1e-8, y_test))) * 100
        print(f"Fold {fold}: RÂ²={r2:.4f}, r={r:.4f}, RMSE={rmse:.3f}, MAPE={mape:.1f}%, Time={time.time()-fold_start:.2f}s")
        r2_list.append(r2); r_list.append(r); rmse_list.append(rmse); mape_list.append(mape)
    exec_time = time.time() - start_time
    print(f"Avg RÂ²={np.mean(r2_list):.4f}, Avg r={np.mean(r_list):.4f}, Avg RMSE={np.mean(rmse_list):.3f}, Avg MAPE={np.mean(mape_list):.1f}%")
    results.append({"Dataset":dataset_name,"Model":"Stacking-GBR","Avg_R2":np.mean(r2_list),"Avg_r":np.mean(r_list),
                    "Avg_RMSE":np.mean(rmse_list),"Avg_MAPE":np.mean(mape_list),"Execution_Time":exec_time})

    # Save intermediate results after each dataset
    pd.DataFrame(results).to_excel("All_Bayes_Results.xlsx", index=False)
    print(f"\nðŸ’¾ Results saved after completing {dataset_name}\n")

# ============================================================
#  RUN PIPELINE FOR ALL DATASETS
# ============================================================
for name in ["g1-PGV", "g2-PGV", "g3-PGV", "g4-PGV", "PGV"]:
    run_models_for_dataset(name)

print("\nâœ… All datasets completed! Final results saved to All_Bayes_Results.xlsx\n")